---
layout: single
title: "AN Image IS WORTH 16X16 WORDS:
TRANSFORMERS FOR Image RECOGNITION AT SCALE" #제목
excerpt : ""
categories: 
    - deeplearningCV #카테고리설정
tag: 
    - ["CV","CNN"] #테그

date: 2024-12-10
last_modified_at: 2024-12-10
classes: wide    
---

[AN Image IS WORTH 16X16 WORDS : TRANSFORMERS FOR Image RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929){: .btn .btn--primary}


# 1. Introduction


## NLP
- **Transformer 기반 architecture**는 NLP에서 사용되며, 대규모 text corpus에서 **pre-training** 후, 작은 작업별 dataset으로 **fine-tuning**하는 방식이 일반적
- Transformer의 **계산 효율성**과 **확장성** 덕분에 1000억 개 이상의 파라미터를 가진 model도 훈련 가능해졌으며, 성능의 한계가 보이지 않고 있음.

## CV
- CNN은 CV에서 여전히 많이 사용
- **Self-Attention**을 CNN에 결합하거나 CNN을 대체하는 연구가 진행
  하지만 이러한 model들은 하드웨어 acceleter에서 효과적으로 확장되지 못해 large-scale Image recognition에서는 ResNet 기반 architecture가 많이 사용

## Vision Transformer (ViT)의 접근
- NLP에서의 Transformer 성공을 기반으로, **Image에 Transformer를 직접 적용**하는 실험을 수행.
- Image를 **patch**로 분할하고 이를 **linear embedding**으로 변환한 후 Transformer에 입력.
- NLP에서의 토큰처럼 Image patch를 처리하며, Image classfication 작업을 **supervised learning**으로 수행.

## 결과
- **middle scale dataset(ImageNet)**에서 regularization 없이 훈련 시 ResNet보다 약간 낮은 정확도
  - 이는 Transformer가 CNN에 비해 **Inductive Bias**이 부족하기 때문:
    - **Translation Equivariance**와 **Locality** 부족으로 인해 데이터가 부족한 경우 일반화가 어려움.
- 그러나 **대규모 dataset(14M~300M Image)**에서 훈련할 경우 성능이 크게 향상
  - ViT는 **ImageNet-21k**와 **JFT-300M**에서  후 여러 Image recognition 벤치마크에서 최첨단 성능을 달성.
  - 주요 성과:
    - **ImageNet:** 88.55%
    - **ImageNet-ReaL:** 90.72%
    - **CIFAR-100:** 94.55%
    - **VTAB (19 tasks):** 77.63%


# 2. Related Work

## Transformer
- Transformer는 Vaswani et al.에 의해 **machine translation**을 위해 제안됨.
- 이후, **NLP의 여러 작업**에서 최첨단 방법으로 자리 잡음.
- 대규모 Transformer model은 **pre-training** 후, 작업별 dataset에 대해 **fine-tuning**됨.
  - **BERT (Devlin et al., 2019):** 노이즈 제거 기반 self supervised learning.
  - **GPT (Radford et al., 2018, 2019; Brown et al., 2020):** 언어 model링 기반 학습.

## Image 처리에서의 Transformer
- Image를 처리하기 위해 단순히 **self-attention**을 적용할 경우, **interaction between pixels**의 계산 비용이 **quadratically** 증가하여 현실적으로 불가능.
- 이를 해결하기 위해 다양한 **approximation methods**이 시도됨:
  
## ViT 이전 유사 연구

- **Cordonnier et al. (2020):**
  - 2×2 크기의 **Image patches**를 추출하여 **self-attention**을 적용.
  - ViT와 유사하지만, ViT는 더 큰 **datasets**에서 **pre-training**을 통해 성능을 극대화.
  - **중간 해상도 Images를 처리**할 수 있는 확장성 제공.

## Image GPT (iGPT)
- **Chen et al. (2020a):**
  - **Transformer**를 pixel에 적용.
  - 해상도와 color space 축소 후, **generative model**로 훈련.
  - 결과적으로 학습된 표현은 **classification** 에서 선형적으로 fine-tuning 가능하며, **ImageNet에서 72% 정확도** 달성.

## ViT의 공헌
- ViT는 ImageNet을 초과하는 **large-scale datasets**에서의 **Image recognition**을 탐구.
- **ImageNet-21k** 및 **JFT-300M** Dataset에 대해 Transformer 기반 접근법 적용:
  - **Mahajan et al. (2018):** CNN의 대규모 Dataset 확장성 연구.
  - **Kolesnikov et al. (2020), Djolonga et al. (2020):** CNN의 transfer learning 효율성 연구와 비교.
  - ViT는 기존 ResNet 기반 model보다 **Transformer 활용**으로 성능 향상.

---

# 3. Method 

## 3.1 Vision Transformer (ViT)
![Image5](/assets/images/SAM/image3.jpg){: .align-center}
### 개요
- ViT는 **original Transformer architecture (Vaswani et al., 2017)**를 최대한 그대로 사용.

### Image 처리
- 2D **Images**를 Transformer에 입력하기 위해, 다음과 같이 변환:
  1. **Image**를 크기 $( (H, W, C) )$로 표현 (**H**: height, **W**: width, **C**: channels).
  2. Image를 크기 $( (P, P) )$의 **patches**로 분할.
  3. **Patches**를 1차원 vector로 flatten하여 **linear embedding** 수행:
     - patch의 총 수 $( N = HW / P^2 )$, Transformer input sequence 길이에 해당.
  4. patch를 $( D )$-차원 공간으로 mapping하는 **trainable linear projection**을 적용하여 **patch embeddings** 생성.

### 학습 과정
- **[class] token** 추가:
  - BERT와 유사하게, patch 임베딩에 학습 가능한 [class] token을 추가.
  - Transformer encoder의 출력에서 [class] token 상태를 사용해 **Image representation** 생성.
  - 이 토큰에 **classification** 작업을 위한 **MLP head** 연결:
    - **Pre-training** 시 1개의 hidden-layer을 가진 MLP.
    - **Fine-tuning** 시 single linear layer.

### Position Embedding
- patch 간의 **positional information**을 보존하기 위해 **1D position embeddings** 추가.
- 더 복잡한 2D-aware positional embedding을 사용할 때 성능 향상이 크지 않음.

### Transformer encoder 구조
  1. **Multi-Head Self-Attention (MSA):** 각 층에서 **Layer Normalization (LN)** 적용, residual connection 추가.
  2. **MLP block:** **GELU non-linearity** 포함 두 개의 layer로 구성.

### 수식
1. input sequence 생성:
   $$
   z_0 = [x_{\text{class}}; x^1_pE; x^2_pE; \cdots; x^N_pE] + E_{\text{pos}}
   $$
2. MSA와 residual connection:
   $$
   z'_\ell = \text{MSA}(\text{LN}(z_{\ell-1})) + z_{\ell-1}, \quad \ell = 1 \ldots L
   $$
3. MLP와 residual connection:
   $$
   z_\ell = \text{MLP}(\text{LN}(z'_\ell)) + z'_\ell, \quad \ell = 1 \ldots L
   $$
4. 최종 출력:
   $$
   y = \text{LN}(z_L^0)
   $$

### Inductive Bias
- ViT는 CNN보다 **Image-specific inductive biases**가 적음:
  - CNN은 **locality**, **2D neighborhood structure**, **translation equivariance**가 각 층에 내재.
  - ViT는 주로 **global self-attention**을 사용하며, 이러한 특성을 학습을 통해 보완.

### hybrid architecture
- CNN의 feature map을 Transformer 입력 sequence로 사용
- feature map에서 추출한 patch에 대해 **linear embedding** 적용.
- feature map의 공간적 크기를 flatten하여 Transformer 차원으로 투영.

---

## 3.2 Fine-tuning and Higher Resolution

### Fine-tuning
- ViT는 **large-scale datasets**에서 **pre-training** 후, 더 작은 down-stream에 transfer learning.
- **Pre-trained** prediction head를 제거하고 **zero-initialized feedforward layer $( D \times K )$** 추가:
  - $K$: down-stream의 class 수.

### higher-resolution에서의 Fine-tuning
  - patch 크기는 유지하며, 결과적으로 입력 sequence 길이가 증가.
  - **2D interpolation**을 사용하여 **pre-trained position embeddings**를 조정.

### ViT의 특징
- ViT는 **memory constraints 내에서 arbitrary sequence length**를 처리 가능.
- positional embedding 및 patch 추출은 **Image의 2D 구조**에 대해 수동으로 Inductive Bias을 주는 유일한 단계.


# 4. Experiments

## 4.1 Setup

### Dataset
- 사용 Dataset:
  - **ImageNet-1k** (1.3M Image, 1,000 Class)
  - **ImageNet-21k** (14M Image, 21,000 Class)
  - **JFT-300M** (303M Image, 18,000 Class)
- transfer learning model은 다양한 벤치마크 Dataset에서 평가:
  - **CIFAR-10/100**, **Oxford-IIIT Pets**, **Oxford Flowers-102**

### model
- **Vision Transformer model**
  - model size:
    - ViT-Base (12 layers, 768 hidden size, 86M params)
    - ViT-Large (24 layers, 1024 hidden size, 307M params)
    - ViT-Huge (32 layers, 1280 hidden size, 632M params)
  - model name:
    - ViT-L/16은 "Large" model로, patch 크기가 $(16 \times 16)$.
- **Baseline CNN (ResNet)**
  - Batch Normalization을 Group Normalization으로 교체.
  - **ResNet (BiT)**로 명명.
- **하이브리드 model**:
  - ResNet의 중간 특징 맵을 Transformer의 input sequence로 사용.

### Training & Fine-tuning
- **Train**:
  - Optimizer: Adam $$((\beta_1 = 0.9, \beta_2 = 0.999))$$, Batch size: 4096.
  - 세부 학습 시 SGD 사용, Batch size: 512.
  - 높은 해상도에서 세부 학습 수행 (ViT-L/16: 512, ViT-H/14: 518).
- **평가 지표**:
  - fine-tuning accuracy, Few-shot accuracy 

---

## 4.2 Comparison to SOTA

### 결과
- **JFT-300M pre trained model** 성능:
  - ViT-L/16은 BiT-L보다 모든 task에서 더 나은 성능을 보이며, 계산량이 적음.
  - ViT-H/14는 더 어려운 Dataset에서도 추가 성능 향상.
- 성능 :
  - **ImageNet**: ViT-H/14 (88.55%), BiT-L (87.54%), Noisy Student (88.4%).
  - **CIFAR-100**: ViT-H/14 (94.55%), BiT-L (93.51%).
  - **VTAB (19 tasks)**: ViT-H/14 (77.63%), BiT-L (76.29%).

---

## 4.3 Pre-training Data Requirements

### Dataset 크기, 성능
- 작은 Dataset (ImageNet-1k)에서는 ViT-Large가 ViT-Base보다 성능이 낮음.
- **JFT-300M**과 같은 대규모 Dataset에서는 model 크기가 클수록 성능 향상.

### Few-shot
- 작은 Dataset에서는 ResNet이 ViT보다 나은 성능.
- Dataset 크기가 커질수록 ViT가 ResNet을 초과하는 성능을 보임.

---

## 4.5 Inspecting Vision Transformer

### 내부 표현 분석
- ViT의 첫 번째 레이어는 patch를 선형적으로 투영하여 낮은 차원의 공간으로 변환.
- **positional embedding**:
  - patch 간 거리를 cosine similarity로 학습.
  - 2D Image의 행렬 구조를 학습하여 공간적 관계를 나타냄.

---

## 4.6 Self-supervision

### 결과
- BERT의 masking 언어 model링을 모방하여 **masking patch 예측** 수행.
- self supervised learning을 통한 ViT-B/16:
  - **ImageNet에서 79.9% 정확도**로, 초기 성능보다 2% 향상.
  - 그러나 supervised learning 성능에 비해 여전히 4% 낮음.

# 5. Conclusion

- 논문에서는 **Transformer를 Image recognition에 직접 적용**하는 방법을 탐구.
- 기존 연구와 달리, ViT는 **Image 특화 Inductive Bias**을 architecture에 도입하지 않음:
- Image를 **patch의 sequence**로 해석하여 **NLP에서 사용하는 표준 Transformer Encoder**로 처리.
