---
layout: single
title: "시퀀스 （Sequence）와 RNN（recurrent neural network)" #제목
categories: 
    - deeplearningNLP #카테고리설정
tag: 
    - ["NLP","Text","RNN"] #테그

date: 2024-01-21
last_modified_at: 2024-01-21
classes: wide    
---

# 시퀀스란?

 순서가 존재하는 항목의 모음이다. 언어는 시계열 자료와 비슷하게 단어가 무작위（iid）하게 배열되는 것이 아닌, 앞과 뒤 데이터에 의존한다. 따라서 자연어 처리를 다루기 위해서는 시퀀스를 반드시 이해해야 한다. 
 
# RNN

 Transformer 등장 전 전통적으로 시퀀스 데이터를 다루었던 딥러닝 모형이다.

![RNN](/assets/images/deeplearning_NLP/RNN.png){: .align-center}

위 구조처럼 순차적으로 시퀀스 표현을 학습한다. RNN은 현재 입력 벡터와 이전 히든 벡터를 통해 히든 벡터를 계산하는 방식으로 모델이 연산되어진다. RNN은 이전 값을 계속 이어서 사용하기 때문에 back propagation 과정에서 Gradient Vanishing 문제가 발생하기 쉽다. 따라서 문장의 길이가 길다면, 성능이 급격하게 떨어진다는 단점이 있다. 이후 LSTM, GRU 등으로 발전하지만 고정된 크기의 context 벡터에 압축을 시켜야하므로 병목 현상이 발생할 수 있다는 단점이 있다.
 
# 이후 시퀀스 연산 모델의 발전

 LSTM과 GRU가 등장하면서 여전히 단점이 존재했기 때문에 매번 히든 vector 값을 가지고 있는 Attention과 함께 사용했지만 여전히 context 벡터로 압축을 해야하므로 단점이 완전히 해결되지 않았다. 그러나 Attention is all you need라는 논문에서 Attention 기법만으로 학습시키는 Transformer 모델이 등장하면서 이 모델이 자연어 처리 분야에서 큰 비중을 가지게 되었다.
