---
layout: single
title: "cs224n Lecture 1 리뷰" #제목
excerpt : ""
categories: 
    - cs224n #카테고리설정
tag: 
    - ["NLP"] #테그

date: 2024-02-19
last_modified_at: 2024-02-19
#classes: wide    
---

[CS224N](https://web.stanford.edu/class/cs224n/){: .btn .btn--primary}

해당 글은 Stanford 대학 2021년 Winter 수업인 CS224n 강의를 듣고 정리한 글입니다. 1주차 수업부터 차례차례 정리할 계획입니다.


Lecture 1 : Introduction and Word Vectors


# 1. How do we represent the meaning of a word?

Lecture 1에서는 먼저 단어의 의미를 컴퓨터로 어떻게 표현할지에 대해 설명하고 있다.

## 1.1 WordNet

![WordNet](/assets/images/cs224n/cs224n-1.png){: .align-center}

WordNet은 전통적인 방법으로 동의어 및 상위 하위 관계의 언어 집합이다. 이 방법은 NLP에서 사용되기에는 부족하다. 그 이유는 인간이 구성한 동의어, 상하위 관계 사전으로 단어가 제한되어, 신조어의 경우 단어를 파악하기 어렵다. 신조어가 생성될 경우 지속적으로 인력이 투입되어야 하므로 유지 비용도 크다는 단점이 있다. 또한 단어의 유사도를 파악하기 어렵고, 뉘양스를 파악하는데 어려움이 있다.

## 1.2 One-Hot Vector

![one-hot](/assets/images/cs224n/cs224n-2.png){: .align-center}

One-Hot Vector는 단어를 간단하게 벡터로 나타내는 방법이다. 위 그림처럼 N개의 단어가 존재할 때, 크기가 N인 벡터를 생성하고, 해당 단어 일 때, 1로 표현한다. One-Hot Vector 방법을 사용하게 되면, 벡터로 단어를 표현하여 컴퓨터가 사용하기 편하다는 장점이 존재하지만, 벡터의 크기가 단어의 개수인 만큼 학습 시 비효율적이고, 두 개의 단어가 직교하여 내적시 값이 1이 나오기 때문에, 두 단어 사이의 관계를 파악하기 어렵다는 단점이 존재한다.

## 1.3 Distributional semantics

이 방법은 언어 데이터를 분포 속성을 기반으로 문맥을 통해 단어 유사성을 벡터로 표현한 방법이다. 

![](/assets/images/cs224n/cs224n-3.png){: .align-center}

고정된 크기의 벡터에 단어들을 표현하는 방법으로 문맥 학습을 통해 단어를 표현한다. 위 그림처럼 banking이라는 단어를 크기가 8인 벡터로 표현한 것을 알 수 있다. 이 방법을 사용하면 벡터의 크기가 One-Hot Vector로 표현할 때보다 작아지므로 연산 속도가 빨라지고, 유사한 단어끼리는 벡터 공간에서 위치가 비슷하므로, 단어 간의 관계를 파악하기 좋으며 NLP 분야에서 많이 사용하고 있는 방법이다.

또한 이 방법은 word embedding 혹은 word representation으로 불리기도 한다.

![](/assets/images/cs224n/cs224n-4.jpg){: .align-center}

위 그림처럼 woed vector 공간에서 유사한 단어끼리 가까운 곳에 위치하는 것을 볼 수 있다.

# 2. Word2vec

Word2vec은 각 단어에 대한 벡터를 스스로 만들어내는 word vector 학습 프레임워크이다.

이 방법의 idea는 다음과 같다.

* 거대한 corpus를 우리가 가지고 있을 때, 모든 단어를 고정된 크기의 vector로 표현한다.
* random vector로 시작하여 각 단어의 vector값을 찾는다.
* 중심 단어를 $c$, 문맥단어를 $o$로 표현한다.
* $c$, $o$의 유사도를 사용하여 $c$가 주어졌을 때, $o$가 나타날 확률을 계산한다.
* 확률을 최대화 하는 vector 값을 찾는다.

![](/assets/images/cs224n/cs224n-5.png){: .align-center}

* 계산 과정

into 단어를 $c$라고 가정하였을 때의 계산과정을 알아보자.

앞, 뒤로 $j$ 개의 단어를 참고한다고 하였을 때, $P(W_{t-j} \mid W_t)$ 부터 $P(W_{t+j} \mid W_t)를 계산한다. 이 과정을 모든 단어에 대해 반복한다.

1) Likelihood
 $L(\theta) = \prod_{t=1}^T \prod_{-m\leq j \leq m, j\neq 0}P(w_{t+j} \mid w_t ; \theta)$

 $ t=1,...N$에 대하여 Likelihood를 최대화 하는  $\theta$를 찾아야한다.


2) objective function
