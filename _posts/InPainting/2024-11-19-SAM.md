---
layout: single
title:  "Segment Anything" #제목
excerpt : ""
categories: 
    - Inpainting #카테고리설정
tag: 
    - ["inpainting", "segmentation"] #테그

use_math: true

date: 2024-11-19
last_modified_at: 2024-11-19
#classes: wide    
---
[Segment Anything](https://arxiv.org/pdf/2304.02643){: .btn .btn--primary}

![Image5](/assets/images/SAM/image1.jpg){: .align-center}

&nbsp;&nbsp; 이 논문에서는 Image segmetation 분야에서 새로운 model, dataset, task를 제공하는 Segment Anything project를 진행한다. 여기서 사용되는 SAM(segment anything model)은 promptable하게 학습되어 zero-shot 전이 학습이 가능하며, 이 모델과 기존에 web-scale dataset이 존재하지 않아 어려웠기 때문에, SA-1B라는 새로운 데이터셋을 제공한다.

# 1.Introduction

web-scale dataset으로 pre-train된 LLM은 **Zero-shot** 및 **Few-shot** 을 통해 NLP 분야를 혁신하고 있다. 이 모델들은 train data와 다른 data의 분포에서도 일반화할 수 있다. 이것은  computer vision **(CLIP, ALIGN)** 에서도 적용하려 시도중이며, 특히 Image Segmentation 문제를 해결하기 위해 **SAM (Segment Anything Model)**이라는 모델이 제안되었다. SAM은 **prompt engineering**을 통해 새로운 데이터 분포에서도 일반화된 Segmentation 작업을 수행할 수 있습니다.

---

## **segmentation을 위한 세가지**
SAM의 개발은 다음 세 가지 핵심 질문에 기반을 두고 있다.

1. **task:** 제로샷 일반화를 가능하게 하는 promptable segmentation task는 무엇인가?  
2. **model:** 유연한 prompt와 빠른 Segmentation를 지원하는 model architecture는 무엇인가?  
3. **data:** 모델을 학습시키기 위한 대규모 dataset은 어디서 얻을 수 있는가?

---

## **Contribution**

### 1. **Promptable Segmentation Task**  
- prompt 기반의 segmentation task  
- 모호한(ambiguity) prompt에서도 합리적인 segmentation mask를 출력  
- 다양한 down stream에 활용 가능, zero-shot 일반화

---

### 2. **SAM (Segment Anything Model)**  
- **module architecture(세가지 제약조건)** 로 설계:  
  - **Image Encoder:** 입력 이미지를 임베딩으로 변환.  
  - **prompt Encoder:** prompt(point, box, mask, text 등)를 임베딩.  
  - **mask decoder:** 이미지와 prompt 임베딩을 결합하여 Segmentation 마스크 생성.  
- **interactive(interactive) 사용:**   
- **amoiguity 처리:** 하나의 prompt로 여러 마스크를 생성하여 다양한 객체를 다루며 모호성 처리.

---

### 3. **Data Engine**  
- 일반화를 위한 SAM을 학습시키기 위해 대규모의 다양한 마스크 데이터를 생성하는 방법  
- 기존 segmentation 데이터셋은 부족하므로, **Model-in-the-loop Data Generation** 을 활용
  1. **assisted manual:** 모델이 annotator를 보조하는 방식으로 데이터 생성.  
  2. **semi-automatic:** 모델이 일부 마스크를 자동 생성하고, annotator는 남은 작업 수행.  
  3. **fully-automatic:** 모델이 모든 마스크를 자동으로 생성.  
- 이를 통해 기존 segmentation 데이터셋보다 400배 많은 마스크 데이터를 확보했다.  

---

### **SA-1B 데이터셋**
- **SA-1B 데이터셋:** 11M의 이미지와 10억 개 이상의 마스크로 구성된 대규모 segementation 데이터셋.  
- **특징:**  
  - 고품질과 다양성을 유지하며 완전 자동화된 방식으로 생성  
  - 기존 segmentation 데이터셋보다 400배 많은 마스크 포함  
- **목표:**  
  - SAM 학습뿐만 아니라 향후 segmentation 연구에 활용될 수 있는 resource 제공

---

### **Responsible AI**
- 데이터셋 및 모델의 공정성과 편향 문제를 연구  
- SA-1B는 다양한 국가 및 경제적 배경을 반영한 데이터를 포함하며, 다양한 집단 간 성능 차이가 적도록 설계  


---

### **Experiments**

- **23개의 segmentation 데이터셋**에서 SAM의 성능 평가.  
  - **Zero-shot Transfer:** 단일 포인트 prompt로 높은 품질의 마스크 생성.  
  - 기존 수동 annotation 데이터와 비슷한 수준의 정확도.  

- **다양한 down stream 작업에서 강력한 성능 :**  
  - edge detection, object proposal generation, instance segmentation 등.  
  - text-to-mask prediction  
- SAM은 train 데이터에 없는 새로운 object 및 image dstribution에도 적용 가능.

---


# **2. Segment Anything Task**

SAM은 NLP에서 "next token prediction"이 사전 학습과 다양한 down stream 작업을 해결하는 데 사용되는 것에 영감을 받았다. 이를 통해 segmentation에 적합한 task를 정의하고 SAM의 prompt 기반 segmentation 작업을 제안한다.

---

## **task**
- **Prompt:** 이미지에서 segmentation 대상을 나타냄  
  - ex) foreground/배경 점, 박스 또는 마스크, 자유 형식 텍스트 등  
- **목표:** 주어진 prompt에 대해 유효한 segmentation mask를 반환  
  - **valid mask:** prompt가 모호하더라도, 적어도 하나의 객체에 대해 합리적인 마스크를 생성  
  - NLP의 언어 모델이 모호한 prompt에 대해 일관된 응답을 생성하는 것과 유사.  
- 이 작업은 자연스러운 사전 학습 알고리즘을 제공하며, zero-shot transfer를 가능하게 함

---

## **Pre-Training**
- **과정:**  
  1. 각 sample에 대해 다양한 prompt(점, 박스, 마스크 등)를 시뮬레이션
  2. 모델의 마스크 예측 결과를 Ground Truth와 비교  
- **기존 interactive segmentation과 차이점:**  
  - interactive segmentation은 사용자의 입력을 통해 유효한 마스크를 생성하는 것이 목표  
  - SAM은 **모든 prompt**에 대해 즉각적으로 유효한 마스크를 생성하는 것이 목표
- 이 방식은 모호한 prompt를 포함하는 실제 사례에서 효과적인 모델을 학습하도록 보장

---

## **Zero-shot transfer**
- pre-training은 모델이 모든 prompt에 적절히 응답할 수 있는 능력을 제공  
- **prompt Engineering**을 통해 Down Stream 작업 해결 :  
  - ex) 고양이 bounding box detector를 이용해, 해당 박스를 prompt로 제공하여 고양이 instance segmentation 수행  
- 다양한 Segmentation 작업을 prompt로 변환 :  
  - automatic dataset labeling, Edge detection, object proposal generation 등

---

## **Related tasks & 차이점**
- **Segmentation :**  
  - interactive Segmentation, Edge detection, object proposal generation, foreground Segmentation, semantic Segmentation, instance Segmentation, panoptic Segmentation  
- **prompt 기반 Segmentation의 목표:**  
  - 광범위한 Segmentation 작업에 적응 가능한 모델 구축.  
  - **기존 multi-task 모델과 차이점:**  
    - multi-task 모델은 고정된 작업 세트를 처리하도록 설계.  
    - prompt 기반 Segmentation 모델은 학습 시 정의되지 않은 새로운 작업도 수행 가능.  

---

## **Discussion: prompt와 composition**
- **prompt와 composition의 장점:**  
  - 단일 모델을 확장 가능한 방식으로 활용  
  - 모델 설계 시점에 예측하지 못했던 작업도 수행  
- **ex)**  
  - CLIP은 DALL·E의 image-text alignment 구성 요소로 사용  
  - 유사하게, SAM은 더 큰 시스템의 Segmentation 구성 요소로 활용  
- **interactive Segmentation와 비교:**  
  - interactive Segmentation는 인간과의 상호작용을 위해 설계.  
  - prompt 기반 Segmentation 모델은 알고리즘 시스템에 통합 가능.  

---  

# **3. Segment Anything Model (SAM)**

Segment Anything Model (SAM)은 prompt 기반 Segmentation를 위해 설계된 모델로, **이미지 인코더**, **prompt 인코더**, **마스크 디코더**의 세 가지 주요 구성 요소로 이루어져 있습니다. 아래에서 각 구성 요소를 설명합니다.

---

## **SAM의 구성 요소**
### 1. **이미지 인코더 (Image Encoder)**
- **역할:** 이미지에서 고해상도 임베딩 생성.
- **구조:**  
  - MAE [47]로 사전 학습된 Vision Transformer (ViT) [33] 사용.
  - 고해상도 입력 처리를 위해 최소한으로 수정된 구조 채택 [62].
- **작동 방식:**  
  - 이미지당 한 번 실행되며, prompt 적용 전에 미리 계산 가능.

---

### 2. **prompt 인코더 (Prompt Encoder)**
- **역할:** 다양한 유형의 prompt(점, 박스, 텍스트, 마스크 등)를 임베딩.
- **prompt 유형:**  
  1. **희소 prompt (Sparse Prompts):** 점, 박스, 텍스트.  
     - 점과 박스: 위치 인코딩 [95]과 학습된 prompt 타입 임베딩의 합으로 표현.  
     - 텍스트: CLIP [82]의 텍스트 인코더를 사용.  
  2. **밀집 prompt (Dense Prompts):** 마스크.  
     - 컨볼루션을 사용해 임베딩을 생성하고, 이미지 임베딩과 요소별 합산.

---

### 3. **마스크 디코더 (Mask Decoder)**
- **역할:** 이미지 임베딩, prompt 임베딩, 출력 토큰을 기반으로 마스크 생성.
- **구조:**  
  - Transformer 디코더 블록 [103]의 수정된 버전을 사용.  
  - 동적 마스크 예측 헤드를 통해 마스크 생성.  
  - **양방향 주의 메커니즘:**  
    - prompt → 이미지 임베딩.  
    - 이미지 임베딩 → prompt.  
  - 출력 토큰을 동적 선형 분류기에 매핑하여 각 위치에서 마스크 foreground 확률 계산.
- **업샘플링:**  
  - 두 블록 실행 후 이미지 임베딩 업샘플링 및 MLP를 통해 최종 마스크 계산.

---

## **모호성 해결**
- **문제:** 모호한 prompt는 여러 유효한 마스크를 생성할 수 있음.
- **해결 방법:**  
  - 하나의 prompt에 대해 여러 개의 출력 마스크를 생성하도록 모델 설계.  
  - 보통 3개의 마스크(전체, 부분, 하위 부분)로 충분.  
  - 학습 시, 최소 손실(minimum loss)을 역전파 [15, 45, 64].  
  - 모델은 각 마스크에 대해 신뢰 점수(예: IoU)를 예측해 순위 결정.

---

## **효율성**
- 사전 계산된 이미지 임베딩을 기반으로, **prompt 인코더**와 **마스크 디코더**가 작동.  
- **웹 브라우저에서 CPU로 실행 가능:** 약 **50ms** 소요.  
- **실시간 인터랙티브 prompt**를 지원하는 효율적인 설계.

---

## **손실 함수 및 학습**
- **손실 함수:**  
  - Focal Loss [65]와 Dice Loss [73]의 선형 결합 사용 [14].  
- **학습 방식:**  
  - 다양한 기하학적 prompt를 혼합해 학습.  
  - interactive Segmentation를 시뮬레이션하여 11회 반복으로 prompt 샘플링 [92, 37].  
  - 데이터 엔진(§4)과의 통합을 위해 설계.

--- 

# **4. Segment Anything Data Engine**

Segment Anything Data Engine은 **1.1B 마스크 데이터셋(SA-1B)**를 구축하기 위해 설계된 데이터 생성 시스템입니다. 데이터 엔진은 세 가지 주요 단계를 통해 마스크를 수집합니다:  
1. **모델 보조 수동 주석 단계**  
2. **반자동 단계**  
3. **완전 자동화 단계**

각 단계를 아래에 자세히 설명합니다.

---

## **1. 모델 보조 수동 주석 단계 (Assisted-manual Stage)**

### **개요**
- 전문가 주석자가 SAM 기반 도구를 사용해 이미지 내 마스크를 생성.
- 브라우저 기반 인터페이스에서 실시간으로 작동하며 인터랙티브한 주석 경험 제공.
- **주요 특징:**
  - foreground/배경 객체를 클릭하거나, "브러쉬" 및 "지우개" 도구로 픽셀 정밀도 조정 가능.
  - semantic 제약 없음: "사물(things)"과 "재질(stuff)" 모두 자유롭게 주석 가능.

### **진행 방식**
- **초기 데이터:** 공개 Segmentation 데이터셋을 기반으로 SAM을 사전 학습.  
- **반복적 학습:** 새로 주석된 마스크만으로 SAM을 재학습하며 모델 성능 개선.  
- **변화:**  
  - 모델 개선으로 평균 마스크 주석 시간 감소: **34초 → 14초**.  
  - 이미지당 마스크 수 증가: **20 → 44**.  

### **결과**
- **수집 데이터:** 12만 개 이미지에서 총 **430만 개 마스크** 수집.  
- **효율성:** COCO 데이터셋 [66]의 마스크 주석 대비 **6.5배 빠름**.

---

## **2. 반자동 단계 (Semi-automatic Stage)**

### **개요**
- 마스크의 다양성을 높이기 위해 자동 생성된 마스크와 수동 주석을 병합.
- 덜 눈에 띄는 객체에 주목하도록 설계.

### **진행 방식**
1. **자동 검출:** 1단계에서 생성된 마스크를 기반으로 객체를 탐지.  
   - 범용 "객체" 범주를 사용해 바운딩 박스 검출기를 학습.  
2. **주석자 작업:** 자동 생성된 마스크가 포함된 이미지를 제공하고, 누락된 객체를 추가 주석.  
3. **반복적 학습:** 주기적으로 새로운 데이터를 추가하여 SAM을 5회 재학습.

### **결과**
- **수집 데이터:** 추가로 **590만 개 마스크**와 18만 개 이미지 확보.  
  - 총 **1,020만 개 마스크**.
- **효율성:** 자동 마스크 제외 시, 평균 주석 시간은 **34초**로 증가.  
  - 이미지당 마스크 수: **44 → 72**(자동 마스크 포함).

---

## **3. 완전 자동화 단계 (Fully Automatic Stage)**

### **개요**
- 충분한 데이터 축적과 모델 개선을 통해 완전 자동화된 마스크 생성 단계로 전환.

### **핵심 기술**
1. **모호성 인식 모델(Ambiguity-aware Model):**  
   - 모델이 단일 점에서 유효한 여러 마스크를 예측 가능.
   - 32×32 점 격자(prompt grid)를 사용해 각 점에서 객체를 탐지.  
   - **모델 출력:** 하위 부분, 부분, 전체 객체 포함 마스크 생성.  
2. **마스크 안정성 검증:**  
   - 확률 맵 임계값(0.5 ± δ)을 사용해 유사성을 확인하고 안정적인 마스크만 선택.  
3. **후처리:**  
   - 비최대 억제(NMS)를 통해 중복 제거.  
   - 작은 마스크 품질 향상을 위해 이미지의 여러 겹치는 크롭을 처리.

### **결과**
- **수집 데이터:** 총 **1,100만 개 이미지**에서 **11억 개 마스크** 생성.  
- **고품질 마스크:** 안정성과 정확성을 유지하며 방대한 데이터셋 구축.

---

## **결론**
Segment Anything Data Engine은 Segmentation 데이터가 부족한 환경에서도 대규모 데이터셋(SA-1B)을 구축하며, SAM 모델의 성능 향상과 다목적 Segmentation 가능성을 강화했습니다. 이 데이터 엔진은 interactive 도구에서 자동화 시스템으로 발전하며, Segmentation 작업의 효율성을 극대화했습니다.


# **5. Segment Anything Dataset (SA-1B)**

SA-1B는 **11억 개의 고품질 Segmentation 마스크**와 **1,100만 개의 고해상도 이미지**로 구성된 대규모 데이터셋입니다. 이 데이터는 Segment Anything Data Engine을 사용하여 생성되었으며, 컴퓨터 비전의 기초 모델 개발을 촉진하기 위해 설계되었습니다.  
아래에서는 SA-1B의 구성 요소, 마스크 품질 분석, 및 기존 데이터셋과의 비교를 정리합니다.

---

## **이미지 (Images)**

### **특징**
- **총 이미지 수:** 1,100만 장  
- **해상도:** 평균 **3300×4950 픽셀**의 고해상도 이미지.  
  - 배포본은 짧은 변을 1500픽셀로 다운샘플링하여 접근성과 저장 용량 문제를 완화.  
  - 다운샘플링 후에도 COCO [66] 데이터셋의 약 3배 해상도.  
- **프라이버시 보호:** 얼굴과 차량 번호판은 흐리게 처리.

---

## **마스크 (Masks)**

### **특징**
- **총 마스크 수:** 11억 개.  
  - **99.1%**는 완전 자동화 방식으로 생성.  
- **자동 마스크 품질:**  
  - **500개 이미지(5만 개 마스크)**를 샘플링해 전문가가 수정.  
  - 자동 마스크와 전문가 수정 마스크 간 평균 IoU(Intersection over Union):  
    - **90% 이상:** 94%.  
    - **75% 이상:** 97%.  
  - 기존 연구에서 주석자 간 일치도(IoU): **85~91%** [44, 60].  
- **결론:** 자동 생성 마스크의 품질은 높으며, 모델 학습 시 수동 생성 마스크와 비슷한 효과.

---

## **마스크의 속성 분석 (Mask Properties)**

### **객체 중심 분포 (Spatial Distribution of Object Centers)**
- SA-1B는 LVIS v1 [44] 및 ADE20K [117]보다 **이미지 구석**을 더 잘 커버.  
- COCO [66]와 Open Images V5 [60]는 **중심 편향(center bias)**이 더 뚜렷.  

### **데이터셋 크기 비교**
- SA-1B는 Open Images 대비:  
  - **11배 더 많은 이미지**.  
  - **400배 더 많은 마스크**.  
  - **36배 더 많은 이미지당 마스크 수**.  
- ADE20K와 비교해도 **3.5배 더 많은 이미지당 마스크 수**.  

### **이미지 대비 마스크 크기**
- SA-1B는 더 많은 마스크를 포함하므로 **작고 중간 크기 마스크**의 비율이 더 큼.  

### **형상 복잡도 (Shape Complexity)**
- 마스크의 볼록 껍질(convex hull) 대비 복잡도를 분석한 결과,  
  - 다른 데이터셋과 유사한 분포를 보임.  
  - 마스크 크기를 기준으로 계층적 샘플링(stratified sampling)하여 분석.  

---

## **결론**

### **SA-1B의 강점**
1. **대규모 데이터셋:** 기존 데이터셋을 능가하는 크기와 다양성.  
2. **고품질 마스크:** 자동 생성된 마스크도 수동 생성 마스크에 준하는 품질.  
3. **향상된 객체 커버리지:** 이미지의 다양한 영역을 포함하는 더 균형 잡힌 데이터 분포.  

SA-1B는 컴퓨터 비전의 기초 모델 연구에 크게 기여할 것으로 기대되며, 특정 연구 목적으로 사용할 수 있도록 유리한 라이선스 조건 하에 제공될 예정입니다.


# **6. Segment Anything RAI Analysis**

Segment Anything (SAM) 및 SA-1B에 대한 **책임 있는 AI (Responsible AI)** 분석은 데이터셋의 지리적·소득 기반 대표성과 SAM의 공정성을 중심으로 진행되었습니다. 이 과정에서 특정 속성(성별, 연령, 피부 톤) 간의 성능 차이를 평가하며, 결과와 시사점을 아래에 정리합니다.

---

## **1. 지리적 및 소득 기반 대표성 (Geographic and Income Representation)**

### **분석 방법**  
- 이미지를 촬영한 국가를 표준 방법(§C 참조)을 통해 추정.  
- 국가별 이미지 수 및 지역별 소득 수준 비교.  

### **결과**
- **SA-1B의 지역별 이미지 분포:**  
  - 상위 3개 국가는 서로 다른 대륙에 위치.  
  - **유럽**과 **아시아·오세아니아** 지역에서 COCO와 Open Images 대비 이미지 비율이 높음.  
  - **아프리카** 및 **저소득 국가**는 여전히 과소대표.  
    - 그러나 SA-1B는 모든 지역(아프리카 포함)에 대해 최소 **2800만 개 마스크**를 포함.  
    - 이는 기존 데이터셋의 전체 마스크 수보다 10배 많음.  

- **마스크 수 일관성:**  
  - 지역 및 소득 수준에 따라 이미지당 평균 마스크 수가 일정 (94~108개).  

---

## **2. 사람 Segmentation에서의 공정성 (Fairness in Segmenting People)**

### **분석 대상 속성**  
- **성별 표현 (Gender Presentation)**  
- **연령대 (Age Group)**  
- **피부 톤 (Skin Tone)**  

### **데이터 및 방법**  
- 성별 및 연령: **MIAP (More Inclusive Annotations for People) [87]** 사용.  
- 피부 톤: 독점 데이터셋 사용. Fitzpatrick 피부 유형 (1=가장 밝음, 6=가장 어두움)으로 분류.  
- **분석 방법:**  
  - 1~3개 랜덤 샘플링 점을 사용하는 시뮬레이션 기반 상호작용 Segmentation.  
  - 그룹 간 성능 차이 측정.

### **결과 요약**

#### **a. 성별 표현**
- 기존 연구에 따르면 여성은 탐지 및 Segmentation 데이터셋에서 과소대표되는 경향 [115].  
- SAM은 그룹 간 유의미한 성능 차이를 보이지 않음.  

#### **b. 연령대**
- 기존 대규모 데이터셋에서 어린 연령대 및 고령층이 과소대표 [110].  
- SAM은 고령층으로 인식되는 그룹에서 성능이 가장 좋음 (다만 신뢰 구간이 큼).  

#### **c. 피부 톤**
- 대규모 데이터셋에서 밝은 피부 톤이 과대표되고 어두운 피부 톤이 과소대표되는 경향 [110].  
- SAM은 피부 톤 그룹 간 유의미한 성능 차이를 보이지 않음.  

### **추가 분석**
- 의류 Segmentation에서는 성별 표현에 따라 편향이 나타날 가능성 확인 (§C 참조).  

---

## **결론 및 시사점**

1. **대표성 보장:**  
   - SA-1B는 대규모 데이터를 통해 다양한 지역과 소득 수준을 어느 정도 포괄.  
   - 그러나 일부 지역(아프리카)과 저소득 국가의 대표성은 추가 개선 필요.  

2. **공정성 유지:**  
   - SAM은 주요 속성(성별, 연령, 피부 톤)에서 성능 차이를 보이지 않음.  
   - 이는 Segmentation 작업의 특성에서 기인할 가능성 있음.  

3. **미래 고려 사항:**  
   - SAM이 더 큰 시스템의 구성 요소로 사용될 경우 편향이 발생할 가능성 존재.  
   - 특정 응용 분야(예: 의류 Segmentation)에서의 편향을 지속적으로 모니터링 필요.  

# **7. 제로샷 전이 실험**

이 섹션에서는 Segment Anything Model (SAM)의 **제로샷 전이** 실험을 다룹니다. SAM이 학습되지 않은 작업과 데이터셋에서 얼마나 잘 일반화할 수 있는지를 평가하며, 이를 통해 SAM의 다양한 컴퓨터 비전 작업에서의 유연성을 보여줍니다. 주요 실험과 결과를 아래에 정리했습니다.

---

## **7.1 제로샷 단일 포인트 유효 마스크 평가**

### **작업 개요**
- **목표:** 단일 foreground 포인트로부터 객체를 분할.
- **도전 과제:**  
  - 하나의 포인트는 여러 객체를 지칭할 수 있어 모호할 가능성이 있음.  
  - 대부분의 데이터셋에서 모든 가능한 마스크를 포함하지 않음.  

### **평가 지표**
- **자동 평가:** 평균 교차 영역 비율(mIoU).  
- **사람 평가:** 주석자가 마스크 품질을 1점(무의미)에서 10점(픽셀 단위 완벽)으로 평가.

### **데이터셋**
- **23개의 다양한 데이터셋**으로 구성된 세트(예: 수중 이미지, 1인칭 이미지 등).  

### **결과**
1. **mIoU (자동 평가):**
   - SAM은 23개 데이터셋 중 16개에서 RITM(강력한 기준점)보다 높은 점수를 기록.
   - 모호성을 해결하는 오라클(oracle) 방식에서는 SAM이 모든 데이터셋에서 RITM을 능가.

2. **사람 평가:**
   - 주석자는 SAM의 마스크를 RITM보다 일관되게 높게 평가.
   - SAM의 평균 점수는 **7~9점**으로, 높은 식별성과 최소한의 오류를 나타냄.  

3. **추가 결과:**
   - 포인트 개수가 증가할수록 성능이 향상되며, SAM과 기준점 간의 차이는 좁아짐.
   - 랜덤 포인트 샘플링에서도 SAM은 기준점을 능가.

---

## **7.2 제로샷 엣지 감지**

### **작업 개요**
- **목표:** SAM의 마스크 예측 기능을 활용해 이미지에서 엣지를 감지.
- **데이터셋:** BSDS500 (엣지 감지 벤치마크).  

### **방법론**
- SAM에 그리드 형태의 포인트를 prompt로 입력.  
- 마스크 확률 맵을 소벨(Sobel) 필터로 변환하고 NMS(비최대 억제)를 적용해 엣지 맵 생성.  

### **결과**
- **정성적 결과:** SAM은 BSDS500에 주석되지 않은 엣지를 포함해 합리적인 엣지를 예측.
- **정량적 결과:**  
  - 높은 재현율(50% 정확도에서의 재현율, R50)을 기록하지만, 정밀도는 상대적으로 낮음.  
  - 최신 엣지 감지 방법보다는 뒤처지지만, 초기 딥러닝 방법(HED)보다 우수.

---

## **7.3 제로샷 객체 제안**

### **작업 개요**
- **목표:** SAM의 자동 마스크 생성 파이프라인을 활용해 객체 제안을 생성.
- **데이터셋:** LVIS v1 (카테고리 수가 많아 도전적).  

### **결과**
- SAM은 특히 **중간/큰 객체**, **희귀/일반 객체**에서 강력한 성능을 보임.
- **작은 객체**와 **빈번한 카테고리**에서는 약간 뒤처짐. 이는 데이터셋 고유의 편향 때문.

---

## **7.4 제로샷 instance 분할**

### **작업 개요**
- **목표:** 객체 감지기의 출력을 prompt로 사용해 SAM을 instance 분할 시스템에 통합.

### **결과**
- **정량적 결과 (마스크 AP):** SAM은 COCO 및 LVIS 데이터셋에서 ViTDet에 근접한 성능을 보임.
- **정성적 결과:** SAM은 ViTDet보다 경계가 더 선명한 마스크를 생성하며, 사람 평가에서도 우위를 점함.

---

## **7.5 제로샷 텍스트-투-마스크**

### **작업 개요**
- **목표:** 자유 형식의 텍스트 prompt를 기반으로 객체를 분할.
- **방법:** CLIP 텍스트 임베딩을 SAM의 입력 prompt로 활용하도록 수정.

### **결과**
- **정성적 결과:** 간단한 텍스트("바퀴")부터 복잡한 텍스트("비버 이빨 그릴")까지 효과적으로 분할.
- **견고성:** 텍스트 prompt만으로 실패할 경우, 추가 포인트 prompt가 예측을 수정.

---

## **7.6 분석 (Ablations)**

### **결과**
1. **훈련 데이터 구성:**  
   - 자동 생성 마스크만 사용해도 성능은 약간 감소(0.5 mIoU)하지만, 훈련 과정이 간소화됨.

2. **데이터 양:**  
   - 11M 이미지에서 1M 이미지로 줄여도 강력한 성능 유지, SAM의 효율성을 입증.

3. **이미지 인코더 스케일링:**  
   - ViT-H는 ViT-B 대비 큰 향상을 보였으나, ViT-L 대비 개선은 미미.

---

## **결론**
SAM은 엣지 감지와 같은 저수준 작업부터 텍스트-투-마스크와 같은 고수준 작업에 이르기까지 놀라운 제로샷 성능을 보여줍니다. 다양한 작업에서의 유연성과 견고성을 갖추고 있지만, 모호성 해결 및 데이터셋 편향에 대한 의존은 개선 가능성이 있는 부분으로 보입니다.
