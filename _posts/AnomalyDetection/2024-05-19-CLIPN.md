---
layout: single
title: "CLIPN for Zero-Shot OOD Detection paper review" #제목
excerpt : ""
categories: 
    - anomalydetection #카테고리설정
tag: 
    - ["image","anomaly", "ood","noisy label"] #테그

date: 2024-05-19
last_modified_at: 2024-05-19
#classes: wide    
---

[CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No](https://arxiv.org/pdf/2308.12213){: .btn .btn--primary}

# 0. Abstract

OOD detection은 in-distribution을 학습하는 방법을 선호하고, CNN, transformer 기반의 방법들이 사용되었다.

그러나 CLIP기반의 OOD detection은 오직 Class name만을 필요로 한다.
이 논문에서는 OOD detection 방법으로 CLIPN(CLIP saying no)를 제시하고 있다.

이 방법의 핵심은 OOD와 ID sample을 positive semantic prompt와 negative semantic으로 구분할 수 있다는 것이다.

핵심 구성은 다음과 같다.

* 2가지 loss funtion
1. the image-text binary-opposite loss
2. the text semantic-opposite loss

* 2가지 threshold-free inference
1. uilizing negation semantics from 'no' prompts
2. the text encoder

# 1. Introduction

딥러닝은 train과 test dataset의 class가 같을 때(closed world condition), 좋은 성능을 보였지만, real-world에서는 좋은 성능을 보이기 어려운 경우가 있다.
그 이유는 현실 세계에 unknown classes가 많이 존재하기 때문이다.

그래서 OOD Detection은 unknown classes를 구분할 수 있는 것을 목표로 발전해왔다.

그 중 하나의 mainstream이 ID-specific features를 학습하고 input data를 ID class에 얼마나 가까운지 매칭 시키는 방법이다.

![Image1](/assets/images/anomalydetection/CLIPN/image1.png)

그러나 이방법도 위 그림 (a)처럼 초록 별에 위치한 sample들은 잘 구분할 수 있지만, class에 가까이 위치하는 갈색 별은 구분하기가 어렵기 때문에 좋은 정확도를 가지지 못한다.

그래서 최근에는 CLIP을 이용하여 OOD 문제를 해결하려는 방법들이 있고, zero-shot OOD detection으로 확장되었다.

여기서 나온 기법이 ZOC, MCM인데 ZOC는 ImageNet-1K같은 거대 dataset에서 좋은 성능을 보이지 못했고, MCM 역시 hard to-distinguish OOD sample에서 좋은 성능을 보이지 못했다.

