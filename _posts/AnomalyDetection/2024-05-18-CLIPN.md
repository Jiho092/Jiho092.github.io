---
layout: single
title: "CLIPN for Zero-Shot OOD Detection paper review" #제목
excerpt : ""
categories: 
    - anomalydetection #카테고리설정
tag: 
    - ["image","anomaly", "ood"] #테그

date: 2024-05-18
last_modified_at: 2024-05-18
#classes: wide    
---

[CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No](https://arxiv.org/pdf/2308.12213){: .btn .btn--primary}

# 0. Abstract

OOD detection은 in-distribution을 학습하는 방법을 선호하고, CNN, transformer 기반의 방법들이 사용되었다.

그러나 CLIP기반의 OOD detection은 오직 Class name만을 필요로 한다.
이 논문에서는 OOD detection 방법으로 CLIPN(CLIP saying no)를 제시하고 있다.

이 방법의 핵심은 OOD와 ID sample을 positive semantic prompt와 negative semantic으로 구분할 수 있다는 것이다.

핵심 구성은 다음과 같다.

* 2가지 loss funtion
1. the image-text binary-opposite loss
2. the text semantic-opposite loss

* 2가지 threshold-free inference
1. uilizing negation semantics from 'no' prompts
2. the text encoder

# 1. Introduction

딥러닝은 train과 test dataset의 class가 같을 때(closed world condition), 좋은 성능을 보였지만, real-world에서는 좋은 성능을 보이기 어려운 경우가 있다.
그 이유는 현실 세계에 unknown classes가 많이 존재하기 때문이다.

그래서 OOD Detection은 unknown classes를 구분할 수 있는 것을 목표로 발전해왔다.

그 중 하나의 mainstream이 ID-specific features를 학습하고 input data를 ID class에 얼마나 가까운지 매칭 시키는 방법이다.

![Image1](/assets/images/anomalydetection/CLIPN/image1.png)

그러나 이방법도 위 그림 (a)처럼 초록 별에 위치한 sample들은 잘 구분할 수 있지만, class에 가까이 위치하는 갈색 별은 구분하기가 어렵기 때문에 좋은 정확도를 가지지 못한다.

그래서 최근에는 CLIP을 이용하여 OOD 문제를 해결하려는 방법들이 있고, zero-shot OOD detection으로 확장되었다.

여기서 나온 기법이 ZOC, MCM인데 ZOC는 ImageNet-1K같은 거대 dataset에서 좋은 성능을 보이지 못했고, MCM 역시 hard to-distinguish OOD sample에서 좋은 성능을 보이지 못했다.

**이에 대한 해결책으로 논문에서는 CLIPN 모델을 제시한다.**

![Image1](/assets/images/anomalydetection/CLIPN/image2.png)

그림 (b)처럼 일반 CLIP에서는 No logic이 부족하기 때문에 정확도가 떨어지는 것을 확인할 수 있다.

그래서 제시한 방법이 CLIPN이다.

CLIPN은 그림처럼 2개의 class가 존재할 때, 4개의 prompt 그룹으로 구성하고, prompt의 기준은 class와 positive, negative이다.

## 1.1. Architecture

CLIP에 No를 장착한 구조로 learnable no prompts와 no text encoder가 존재한다.

## 1.2. Training Loss

2가지 loss를 가지고 있다.

(1). image-text binary-opposite loss : image feature를 no prompt feature와 매치시킨다.(CLIP에게 no를 가르친다.)

(2). semantic-opposite loss : standard prompt와 no prompt를 서로 멀리 embedding 시킨다.(CLIP에게 no를 이해시킨다.)

## 1.3. Treshold-free Inference Algorithm

(1). competing-to-win : standard, no text encoder에서 최종 예측을 선택

(2). agreeing-to-differ : standard, no text encoder를 모두 고려하면서 OOD class에 대한 추가적인 확률 생성


# 2. Related Work

생략

## 2.1. Contrastive Vision-Language Models

## 2.2 CLIP-based Zero-Shot Learning

## 2.3 Out-of-Distribution Detection

# 3. Methodolgy

## 3.1 Preliminary: CLIP-based OOD Detection

CLIP은 Zero-shot OOD detection에서 거대한 dataset을 사용하여 좋은 성능을 보였다.

CLIP의 image-encoder를 사용하여 image feature를 추출한다. 그러나 CLIP에는 classifier가 존재하지 않는다.
그래서 class ID 이름을 text-encoder에 넣어 text feature를 뽑아내고, class별 가중치로 특징을 얻으면서 classifier의 역할을 한다.

최종적으로 MSP(maximum softmax probability)를 사용하여, 이 값이 threshold보다 작다면 OOD로 처리한다.

## 3.2 Overview of CLIPN

### 3.2.1 Model Architecture

모델의 전반적인 구조는 아래 그림과 같다.

![Image1](/assets/images/anomalydetection/CLIPN/image3.png)



## 3.3 Prompt Design


## 3.4 Training Loss Design


## 3.5 Inference algorithm of CLIPN