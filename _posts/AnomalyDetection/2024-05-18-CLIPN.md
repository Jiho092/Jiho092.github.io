---
layout: single
title: "CLIPN for Zero-Shot OOD Detection paper review" #제목
excerpt : ""
categories: 
    - anomalydetection #카테고리설정
tag: 
    - ["image","anomaly", "ood"] #테그

date: 2024-05-18
last_modified_at: 2024-05-18
#classes: wide    
---

[CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No](https://arxiv.org/pdf/2308.12213){: .btn .btn--primary}

# 0. Abstract

OOD detection은 in-distribution을 학습하는 방법을 선호하고, CNN, transformer 기반의 방법들이 사용되었다.

그러나 CLIP기반의 OOD detection은 오직 Class name만을 필요로 한다.
이 논문에서는 OOD detection 방법으로 CLIPN(CLIP saying no)를 제시하고 있다.

이 방법의 핵심은 OOD와 ID sample을 positive semantic prompt와 negative semantic으로 구분할 수 있다는 것이다.

핵심 구성은 다음과 같다.

* 2가지 loss funtion
1. the image-text binary-opposite loss
2. the text semantic-opposite loss

* 2가지 threshold-free inference
1. uilizing negation semantics from 'no' prompts
2. the text encoder

# 1. Introduction

딥러닝은 train과 test dataset의 class가 같을 때(closed world condition), 좋은 성능을 보였지만, real-world에서는 좋은 성능을 보이기 어려운 경우가 있다.
그 이유는 현실 세계에 unknown classes가 많이 존재하기 때문이다.

그래서 OOD Detection은 unknown classes를 구분할 수 있는 것을 목표로 발전해왔다.

그 중 하나의 mainstream이 ID-specific features를 학습하고 input data를 ID class에 얼마나 가까운지 매칭 시키는 방법이다.

![Image1](/assets/images/anomalydetection/CLIPN/image1.png)

그러나 이방법도 위 그림 (a)처럼 초록 별에 위치한 sample들은 잘 구분할 수 있지만, class에 가까이 위치하는 갈색 별은 구분하기가 어렵기 때문에 좋은 정확도를 가지지 못한다.

그래서 최근에는 CLIP을 이용하여 OOD 문제를 해결하려는 방법들이 있고, zero-shot OOD detection으로 확장되었다.

여기서 나온 기법이 ZOC, MCM인데 ZOC는 ImageNet-1K같은 거대 dataset에서 좋은 성능을 보이지 못했고, MCM 역시 hard to-distinguish OOD sample에서 좋은 성능을 보이지 못했다.

**이에 대한 해결책으로 논문에서는 CLIPN 모델을 제시한다.**

![Image1](/assets/images/anomalydetection/CLIPN/image2.png)

그림 (b)처럼 일반 CLIP에서는 No logic이 부족하기 때문에 정확도가 떨어지는 것을 확인할 수 있다.

그래서 제시한 방법이 CLIPN이다.

CLIPN은 그림처럼 2개의 class가 존재할 때, 4개의 prompt 그룹으로 구성하고, prompt의 기준은 class와 positive, negative이다.

## 1.1. Architecture

CLIP에 No를 장착한 구조로 learnable no prompts와 no text encoder가 존재한다.

## 1.2. Training Loss

2가지 loss를 가지고 있다.

(1). image-text binary-opposite loss : image feature를 no prompt feature와 매치시킨다.(CLIP에게 no를 가르친다.)

(2). semantic-opposite loss : standard prompt와 no prompt를 서로 멀리 embedding 시킨다.(CLIP에게 no를 이해시킨다.)

## 1.3. Treshold-free Inference Algorithm

(1). competing-to-win : standard, no text encoder에서 최종 예측을 선택

(2). agreeing-to-differ : standard, no text encoder를 모두 고려하면서 OOD class에 대한 추가적인 확률 생성


# 2. Related Work

생략

## 2.1. Contrastive Vision-Language Models

## 2.2 CLIP-based Zero-Shot Learning

## 2.3 Out-of-Distribution Detection

# 3. Methodolgy

## 3.1 Preliminary: CLIP-based OOD Detection

CLIP은 Zero-shot OOD detection에서 거대한 dataset을 사용하여 좋은 성능을 보였다.

CLIP의 image-encoder를 사용하여 image feature를 추출한다. 그러나 CLIP에는 classifier가 존재하지 않는다.
그래서 class ID 이름을 text-encoder에 넣어 text feature를 뽑아내고, class별 가중치로 특징을 얻으면서 classifier의 역할을 한다.

최종적으로 MSP(maximum softmax probability)를 사용하여, 이 값이 threshold보다 작다면 OOD로 처리한다.

## 3.2 Overview of CLIPN

### 3.2.1 Model Architecture

모델의 전반적인 구조는 아래 그림과 같다.

![Image1](/assets/images/anomalydetection/CLIPN/image3.png)

**CLIPN은 Image-Encoder, Text-Encoder, "no" Text-Encoder 3개의 module을 가지고 있다.**

(1). Image Encoder($\mathsf{\pi}$):

기존 CLIP과 같은 구조와 parameter를 사용한다. 이 논문에서는 ViT-B, ViT-L로 pre-train 시켰고, parameter는 얼린다.(frozen)

(2). (Standard) Text Encoder($\psi$):

Image Encoder와 마찬가지로 CLIP과 같은 구조와 parameter를 사용한다. $\psi$ 의 입력은 하나의 image에 대해 표현한다 (e.g., "a photo with/of/...")

(3). "No" Text Encoder($\psi^{no}$):

pre-trained CLIP의 text encoder를 초기값을 사용한다. 그러나 $\psi$ 와 다른 점은 학습이 가능하다는 점이고 input으로 negative text를 받는 다는 것이다. 이것은 하나의 이미지와 opposite semantic으로 표현된다.

#### Pre-training CLIPN

$\psi^{no}$ 와 $\psi$ 의 input text는 ID, OOD의 속성처럼 반대의 의미이다. (여기서 두가지 loss를 가진다.)


#### Inference Stage

두가지 Treshold가 있다.

(1). the competing-to-win algorithm:

먼저 image를 standard text를 통해 ID class의 확률을 예측한다.
그 후 no text를 사용하여 가장 높은 확률과 비교하여 ID prediction이 올바른지 확인한다.

(2). the agreeing-to-differ algorithm:

OOD class의 확률을 생성한다.
만약 OOD확률이 가장 높다면 해당 image를 OOD로 판단한다.


## 3.3 Prompt Design

CLIP에서는 image가 positve logic으로 묘사된다.

CLIPN에서는 negative logic을 보완하려고 한다.

이 논문에서는 **"no" prompts** 를 추가한다.

image $x$에 대한 text를 $t$라고 하자.

이 때 "no" prompt pool은 다음과 같이 정의된다.

$pool_{no}(t) = (a photo without {t}, a photo not appearing {t}, ..., a photo not containing {t})$

$L$ : handcrafted “no” prompts.

(1). training에서  $pool_{no}(t)$로 부터 random하게 no prompt를 선택하고 no logic $t^{no}$를 생성한다.

(2). token embedding layer $\rho$ 를 사용하여 embedding한다.

(3). learnable parameter $\sigma$ 를 통해 negative keyword를 표현하고, standard token feature $t$ 공간에 결합한다.

## 3.4 Training Loss Design


## 3.5 Inference algorithm of CLIPN

![Image1](/assets/images/anomalydetection/CLIPN/image4.png)

Train 과정에서 다음을 정의한다.

mini-batch with N input pair

* $\mathcal{B} = {(x_i,t_i,t_i^{no})}_{i=1}^{N} \in \mathbb{D}_{clip}$

* mathbb{D}_{clip}$ : $i$-th image, standard-text,no-text pair

* $f_i = \mathsf{Phi}(x_i)$
* $g_i = \psi(t_i)$
* $g_i^{no} = \psi^{no}(t_i^{no})$
* where $f_i, g_i, g_i^{no} \in \mathbb{R}^{1 X D}$
* D : feature dimension

모든 feature는 $L_2$ noramlization 되어있다.

## 3.5.1 Image-Text Binary-Opposite Loss(ITBO)

* $m(x_i, t_j^{no}) = m_{ij}$ = \begin{cases}
1, & \mbox{if }$i=j$\mbox{} \\
3n+1, & \mbox{if }$i \neq j$\mbox{ is odd}
\end{cases}